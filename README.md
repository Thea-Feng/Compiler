# Compiler
Oat compiler project (LLVM)


# Part 1 Micro Language Compiler
More detailed verstion see [instruction](/Compiler/Part%201-instruction.pdf) and [report](Part1/src/report.md)

#### 1. Background Information:
This project focuses on designing and implementing a **compiler frontend** for a simplified **Micro Language**. The goal is to transform Micro Language code into **LLVM Intermediate Representation (IR)** and then compile it into **RISC-V assembly**. The compiled code is executed on a RISC-V emulator to validate the correctness.

#### 2. Goal:
The primary objective is to implement the **compiler frontend**, which includes:
- A **scanner** to identify tokens,
- A **parser** to generate the **abstract syntax tree (AST)**,
- An **intermediate code generator** that produces LLVM IR from the AST.


#### 3. Design:

- **Scanner**:
  The scanner identifies tokens in the Micro Language based on regular expressions. For example, identifiers (IDs) are matched using the regex `[a-zA-Z][a-zA-Z0-9]{0,31}`, while integer literals (INTLITERAL) are matched with `[0-9]+`. Special care is taken for handling negative numbers, where the parser processes the minus sign to avoid conflict in expressions.

- **Parser**:
  The parser is responsible for processing the tokens and generating a **Concrete Syntax Tree (CST)** and an **Abstract Syntax Tree (AST)**. The parsing process follows the context-free grammar (CFG) of the Micro Language and converts it into the appropriate tree structures.

- **Intermediate Code Generator**:
  The LLVM IR is generated by recursively traversing the AST. LLVM instructions are generated as strings, which simplifies the handling of this minimalistic Micro Language. The code generator allocates memory for variables, processes read and write commands, and manages arithmetic operations like addition and subtraction. For example, the generator uses functions like `genRead` for read statements and `genWrite` for write statements, while arithmetic operations are handled by functions like `add` and `sub`.

#### 4. Implementation Details:

- **Special Cases in Scanner**: Negative literals are parsed correctly by treating the minus sign separately in the parser to avoid conflicts.
  
- **Parse Tree and AST**: The **parse tree** is created for each non-terminal symbol, while the AST focuses on key elements like `program`, `statement list`, `READ`, and `WRITE` operations.

- **LLVM IR Generation**: The IR generator produces LLVM instructions by recursively visiting AST nodes. It uses **maps** to allocate registers and variables. LLVM IR is optimized using `opt --O3` and converted to RISC-V assembly using `llc`.

- **Bonus**: The implementation includes error reporting for invalid syntax and optimizes DFA (Deterministic Finite Automaton) to improve scanning efficiency.

#### 5. How to Run the Code:

1. Compile the source code:
   ```
   make all
   ```

2. Run the Micro Language compiler with a sample program:
   ```
   ./compiler ./testcases/test0.m
   ```

3. Generate and visualize the abstract syntax tree (AST):
   ```
   dot -Tpng ./ast.dot -o ./ast.png
   ```

4. Optimize LLVM IR and compile to RISC-V:
   ```
   opt ./program.ll -S --O3 -o ./program_optimized.ll
   llc -march=riscv64 ./program_optimized.ll -o ./program.s
   ```

5. Execute the program on a RISC-V emulator:
   ```
   riscv64-unknown-linux-gnu-gcc ./program.s -o ./program
   qemu-riscv64 -L /opt/riscv/sysroot ./program
   ```

6. Verify the output with `verify_compiler.sh` script for multiple test cases.

This implementation ensures the correctness of the compiler frontend and generates efficient LLVM IR that can be executed on a RISC-V environment.


## Part 2 Oat v.1 Language Scanner (continue to Part 4)
More detailed verstion see [instruction](/Compiler/Part%202-instruction.pdf) and [report](/Compiler/Part2/src/report.md)

#### 1. Background Information:
This project involves designing and implementing two scanners (lexical analyzers) for **Oat v.1 Language**:
- One scanner is generated using **Flex** (a lexical generator).
- The second scanner is implemented **manually** without any lexical generation tools, which is the main challenge of the project.

The goal is to break the Oat program into **tokens** and identify **lexemes** for each token. The implementation requires constructing **NFA (Non-deterministic Finite Automata)** from regular expressions, converting it to **DFA (Deterministic Finite Automata)**, and scanning the input using DFA.

#### 2. Goal:
- To develop two scanners for Oat language:
   1. **Flex-based scanner** for convenience and verification.
   2. **Handwritten scanner** for understanding DFA/NFA concepts.
  
- Implement **NFA to DFA conversion** and optimize the scanner for efficiency.

#### 3. Design:

- **Flex-based Scanner**:
  The scanner uses regular expressions to define tokens such as `ID`, `INTLITERAL`, and `STRINGLITERAL`. It outputs **<token, lexeme>** pairs based on these patterns. This scanner serves as the reference for comparing the output of the handwritten scanner.

- **Handwritten Scanner**:
  This scanner involves:
  - Building **NFA** for each token based on its regular expression.
  - Merging the NFAs for different tokens into one NFA with a single start and end state.
  - Using **subset construction** to convert the NFA into a **DFA** for deterministic scanning.
  - The DFA performs the scanning, identifying the **longest match** and the **most precedent match** for conflicting tokens.

#### 4. Implementation Details:

- **Scanner (Flex)**: 
  - Recognizes tokens using predefined regular expressions and outputs <token, lexeme> pairs.
  - Example of regular expressions:
    - `ID`: `[a-zA-Z][a-zA-Z0-9]*`
    - `INTLITERAL`: `[0-9]+`
    - `STRINGLITERAL`: `\"[^\"]*\"`
  
- **Scanner (Manual)**:
  - **NFA Construction**: Constructs an NFA for each token using regex.
  - **Subset Construction**: Converts the NFA to a DFA using the subset construction algorithm to remove nondeterminism.
  - **DFA Minimization**: Optionally minimizes the DFA by merging equivalent states to make the scanner more efficient.
  - **Longest Match**: The scanner follows the longest match rule to resolve token conflicts, ensuring the most specific token is chosen.

#### 5. How to Run the Code:

1. **Compiling the Project**:
   ```
   make all
   ```

2. **Running the Flex-based Scanner**:
   ```
   ./lexer < ./testcases/test0.oat
   ```

3. **Running the Handwritten Scanner**:
   ```
   ./scanner ./testcases/test0.oat
   ```

4. **Verification**:
   To verify that both scanners produce the same output:
   ```
   ./verify_compiler.sh
   ```

## Part 3 Parsing Techniques
More detailed verstion see [instruction](/Compiler/Part%203-instruction.pdf) and [report](/Compiler/Part3/src/report.md)

#### 1. Background Information:
This project focuses on implementing and understanding **parsing techniques** for the **Micro Language** and **Oat v.1 Language**. The tasks include:
- Resolving ambiguity in the Micro Language grammar,
- Modifying a given grammar to make it **LL(1)** and exploring **LR(0)** parsing,
- Implementing an **LL(1) parser by hand** for the Oat v.1 language.

#### 2. Goal:
- Address ambiguity in the grammar and demonstrate examples of multiple parse trees for the same input.
- Refactor a grammar to be LL(1) compliant and investigate the conflicts that arise in **LR(0) parsing**.
- Implement a **handwritten LL(1) parser** for Oat v.1 language.

#### 3. Design:

- **Resolve Ambiguity (Micro Language)**:
  Ambiguity in grammar arises when there are multiple valid parse trees for the same input. A counterexample like `ID + ID - ID` demonstrates that this string could be parsed in more than one way (e.g., as `ID + (ID - ID)` or `(ID + ID) - ID`).

- **LL(1) Grammar Modification**:
  - To make the grammar **LL(1)**, techniques like **left recursion elimination** and **left factoring** were applied. The grammar was restructured to be compatible with an LL(1) parser, allowing efficient top-down parsing.
  - Example:
    ```
    E  -> T E'
    E' -> + E | ''
    T  -> int T' | ( E )
    T' -> * T | ''
    ```

- **LL(1) and LR(0) Parsing**:
  - The LL(1) grammar was tested using a web demo, and the LL(1) parsing table and **First/Follow sets** were generated.
  - Similarly, the **LR(0) parsing automaton** was explored. The conflicts that arise in LR(0) parsing (e.g., **shift-reduce conflicts**) were identified, particularly for handling operators like `+` and `*`.

- **Handwritten LL(1) Parser for Oat v.1**:
  - The LL(1) parser is implemented to read the grammar file, calculate the **nullable**, **First**, and **Follow sets**, and construct the **LL(1) parsing table**. The parser processes token streams based on the input Oat v.1 language.

#### 4. Implementation Details:

- **Resolving Ambiguity**:
  The ambiguity in Micro Language grammar was resolved by applying left recursion elimination and demonstrating different parse trees for the same string, confirming the grammar's ambiguity.

- **LL(1) Grammar**:
  After refactoring, the LL(1) grammar was successfully processed in the web demo, and both **parsing tables** and **parse trees** were generated step by step.

- **Handwritten LL(1) Parser**:
  The parser reads input token streams and parses them according to the Oat v.1 grammar, simulating the behavior of an LL(1) parser. The parsing steps are printed for debugging, and it handles both syntactically correct and erroneous inputs.

#### 5. How to Run the Code:

1. **Compiling the Parser**:
   ```
   g++ main.cpp -o main -std=c++11
   ```

2. **Running the Parser**:
   ```
   ./main input_grammar.txt test0-tokens.txt
   ```

3. The program will print the parsing steps as it processes the input, similar to the behavior of an LL(1) web demo.

## Part 4 Compiler Frontend for Oat v.1 
More detailed verstion see [instruction](/Compiler/Part%204-instruction.pdf) and [report](/Compiler/Part4/report.md)
#### 1. Background Information:
This project implements the remaining stages of the **compiler frontend** for Oat v.1 Language, focusing on **semantic analysis** and **LLVM IR generation**. It builds on previous work involving scanning, parsing, and AST generation. The project involves two key parts:
- **Semantic Analysis**: Assigns data types and resolves variable scopes.
- **IR Generation**: Converts the augmented AST into LLVM Intermediate Representation (IR) using the LLVMlite library.

#### 2. Goal:
- Implement a **semantic analyzer** to handle variable scopes and data types.
- Use **LLVMlite** to generate the LLVM IR for the Oat language.
- Ensure that the compiler can pass a set of test cases covering basic and advanced scenarios.

#### 3. Design:

- **Semantic Analysis**:
  - This phase ensures that variables are correctly typed and scoped. It identifies variable declarations (e.g., global vs. local variables) and performs type checking.
  - The **SymbolTable** is used to manage scopes. Variables and their types are tracked, and each variable is given a unique name using scope information to avoid conflicts between global and local variables.

- **IR Generation**:
  - The IR generation is handled using the **LLVMlite** library. This part involves creating LLVM IR from the AST by converting variable declarations, function calls, loops, and conditional branches into LLVM instructions.
  - Special care is taken for **variable handling** (e.g., global vs. local variables) and managing memory (allocating and loading variables). Some challenges include converting string types and dealing with references, which require using LLVM's `getelementptr` for pointer arithmetic.

#### 4. Implementation Details:

- **Semantic Analysis**:
  - The semantic analyzer ensures that each variable is declared before use and that the types of variables in expressions are compatible (e.g., no adding of integers and strings).
  - The **scope stack** is used to assign unique names to variables based on their scope, allowing for correct handling of variable shadowing (i.e., local variables that have the same name as global variables).

- **IR Generation**:
  - The IR generator uses the **LLVMlite** API to generate LLVM code. It deals with basic instructions like **alloca**, **load**, and **store** for variable management. Handling more complex constructs like loops and conditional branches is also implemented.
  - A challenge in IR generation was dealing with string types and reference types, which required converting between types using **getelementptr** for memory addressing.
  - Some parts of the IR generation, such as logical operators (`AND`, `OR`), and reference return types were noted as not fully implemented.

#### 5. How to Run the Code:

1. **Compile and Run**:
   ```
   bash ./verify.sh
   ```

2. This script will process the test cases and generate the AST, perform semantic analysis, and generate the corresponding LLVM IR.

3. The IR can be executed on a compatible LLVM environment or further tested using provided test cases.

#### 6. Challenges:
- **Semantic Analysis** was straightforward, but **LLVM IR generation** was more complex due to handling scope and variable management, especially dealing with **string types** and **reference types**.
- The project involved learning the **LLVMlite** API, which required more in-depth understanding compared to manually printing LLVM code as in Part 1.


